import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np

"""
Differential Evolution (DE/rand/1/bin) Algorithm
================================================

üìö ORIGINAL REFERENCES:
-----------------------
Primary Paper:
    Storn, R., & Price, K. (1997).
    "Differential evolution‚Äìa simple and efficient heuristic for global 
    optimization over continuous spaces."
    Journal of global optimization, 11(4), 341-359.

Extended Work:
    Price, K., Storn, R. M., & Lampinen, J. A. (2005).
    "Differential evolution: a practical approach to global optimization."
    Springer Science & Business Media.

üìñ ALGORITHM DESCRIPTION:
-------------------------
DE is a population-based stochastic optimization algorithm that creates new
candidate solutions by combining existing ones according to a simple formula,
and then keeping whichever candidate has the best fitness.

STRATEGY: DE/rand/1/bin
- DE: Differential Evolution
- rand: Base vector is randomly selected
- 1: One difference vector is used
- bin: Binomial crossover

üî¨ MATHEMATICAL FORMULATION:
----------------------------
For each target vector x_i in generation G:

1. MUTATION (Differential Mutation):
   v_i = x_r1 + F ¬∑ (x_r2 - x_r3)
   
   where:
   - r1, r2, r3 are randomly selected indices (r1 ‚â† r2 ‚â† r3 ‚â† i)
   - F is the mutation factor (scale factor) ‚àà [0, 2]

2. CROSSOVER (Binomial Crossover):
   u_i,j = { v_i,j  if rand(0,1) ‚â§ CR or j = j_rand
           { x_i,j  otherwise
   
   where:
   - CR is the crossover probability ‚àà [0, 1]
   - j_rand ensures at least one parameter is mutated

3. SELECTION (Greedy Selection):
   x_i(G+1) = { u_i  if f(u_i) ‚â§ f(x_i)
              { x_i  otherwise

‚öôÔ∏è PARAMETER RECOMMENDATIONS:
-----------------------------
Standard Settings (Storn & Price, 1997):
    - F = 0.5 (mutation factor)
    - CR = 0.9 (crossover rate)
    - pop_size = 10√óD (5√óD to 10√óD is common)

Parameter Tuning Guidelines:
    - F ‚àà [0.4, 1.0]: Lower values for unimodal, higher for multimodal
    - CR ‚àà [0.0, 1.0]: Higher values increase exploration
    - Standard: F = 0.5, CR = 0.9

üí™ STRENGTHS:
-------------
- Simple implementation
- Few control parameters (only F and CR)
- Excellent for continuous optimization
- Self-organizing behavior
- Good for unimodal functions

‚ö†Ô∏è WEAKNESSES:
--------------
- Parameter sensitive (F and CR must be tuned)
- Slow convergence in high dimensions
- May stagnate in highly multimodal landscapes (Rastrigin, Schwefel)
- Performance degrades without proper parameter tuning

üìä COMPUTATIONAL COMPLEXITY:
---------------------------
Time: O(pop_size √ó max_iter √ó dim)
Space: O(pop_size √ó dim)

Args:
    objective_func (callable): Objective function to minimize
    dim (int): Problem dimensionality
    bounds (tuple or list): Search space bounds as (lb, ub) or [(lb, ub)] √ó dim
    pop_size (int): Population size (typically 10√ódim)
    max_iter (int): Maximum number of iterations
    F (float): Mutation factor/scale factor (default: 0.5)
    CR (float): Crossover probability (default: 0.9)
    
Returns:
    tuple: (best_solution, best_fitness, convergence_curve)
        - best_solution: Best solution found
        - best_fitness: Fitness of best solution
        - convergence_curve: Best fitness at each iteration
"""

def de(objective_func, dim, bounds, pop_size=None, max_iter=1000, F=0.5, CR=0.9):
    """
    Differential Evolution - DE/rand/1/bin Strategy
    
    Standard implementation following Storn & Price (1997).
    Parameters F and CR are fixed throughout the evolution.
    
    NOTE: This implementation uses FIXED parameters to demonstrate
    the classical DE behavior and its limitations on multimodal problems.
    Parameter tuning is required for optimal performance on different
    problem types.
    """
    
    # ========================================================================
    # STEP 1: PARAMETER INITIALIZATION
    # ========================================================================
    
    # Default population size: 10√óD (Storn & Price recommendation)
    if pop_size is None:
        pop_size = 10 * dim
    
    # Standardize bounds
    if isinstance(bounds, tuple) and len(bounds) == 2:
        lb = np.full(dim, bounds[0])
        ub = np.full(dim, bounds[1])
    else:
        bounds = np.array(bounds)
        if bounds.shape == (2,):
            lb = np.full(dim, bounds[0])
            ub = np.full(dim, bounds[1])
        else:
            lb = bounds[:, 0]
            ub = bounds[:, 1]
    
    # ========================================================================
    # STEP 2: POPULATION INITIALIZATION
    # ========================================================================
    
    # Initialize population uniformly in search space
    population = np.random.uniform(lb, ub, (pop_size, dim))
    
    # Evaluate initial population
    fitness = np.array([objective_func(ind) for ind in population])
    
    # Find best individual
    best_idx = np.argmin(fitness)
    best_solution = population[best_idx].copy()
    best_fitness = fitness[best_idx]
    
    # Convergence tracking
    convergence_curve = np.zeros(max_iter)
    
    # ========================================================================
    # STEP 3: MAIN EVOLUTION LOOP
    # ========================================================================
    
    for iteration in range(max_iter):
        
        for i in range(pop_size):
            # ================================================================
            # MUTATION: DE/rand/1
            # ================================================================
            # Select three random individuals different from current (i)
            # r1, r2, r3 must all be different from each other and from i
            
            candidates = [idx for idx in range(pop_size) if idx != i]
            r1, r2, r3 = np.random.choice(candidates, 3, replace=False)
            
            # Create mutant vector: v_i = x_r1 + F * (x_r2 - x_r3)
            mutant = population[r1] + F * (population[r2] - population[r3])
            
            # Ensure mutant is within bounds (boundary constraint handling)
            mutant = np.clip(mutant, lb, ub)
            
            # ================================================================
            # CROSSOVER: Binomial Crossover
            # ================================================================
            # Create trial vector by mixing target and mutant
            
            trial = population[i].copy()
            
            # Generate random crossover mask
            cross_points = np.random.rand(dim) < CR
            
            # Ensure at least one parameter comes from mutant (original DE rule)
            j_rand = np.random.randint(dim)
            cross_points[j_rand] = True
            
            # Apply crossover
            trial[cross_points] = mutant[cross_points]
            
            # ================================================================
            # SELECTION: Greedy Selection
            # ================================================================
            # Keep better individual between target and trial
            
            trial_fitness = objective_func(trial)
            
            # If trial is better or equal, replace target
            if trial_fitness <= fitness[i]:
                population[i] = trial
                fitness[i] = trial_fitness
                
                # Update global best if necessary
                if trial_fitness < best_fitness:
                    best_fitness = trial_fitness
                    best_solution = trial.copy()
        
        # Record best fitness for this iteration
        convergence_curve[iteration] = best_fitness
        
        # Progress indicator (every 10% of iterations)
        if (iteration + 1) % (max_iter // 10) == 0 or iteration == 0:
            print(f"Iter {iteration + 1:4d}/{max_iter}: Best = {best_fitness:.6e}, "
                  f"F = {F:.2f}, CR = {CR:.2f}")
    
    return best_solution, best_fitness, convergence_curve


# ============================================================================
# TEST CODE - Comprehensive Benchmark Testing
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print(" " * 15 + "DE ALGORITHM - BENCHMARK TESTS")
    print("=" * 70)
    
    # Import benchmark functions
    from benchmarks.sphere import sphere
    from benchmarks.rastrigin import rastrigin
    from benchmarks.ackley import ackley
    from benchmarks.rosenbrock import rosenbrock
    from benchmarks.schwefel import schwefel
    
    # Algorithm parameters
    dim = 30
    pop_size = 10 * dim  # Standard: 10√óD
    max_iter = 1000
    
    # DE-specific parameters
    # NOTE: These are STANDARD PARAMETERS from literature
    # They demonstrate DE's limitations on multimodal problems
    F = 0.5   # Mutation factor (standard value)
    CR = 0.9  # Crossover rate (standard value)
    
    # Test suite with CORRECT bounds
    benchmarks_to_test = [
        {
            "name": "Sphere",
            "func": sphere,
            "bounds": (-100, 100),
            "optimal": 0.0
        },
        {
            "name": "Rastrigin",
            "func": rastrigin,
            "bounds": (-5.12, 5.12),
            "optimal": 0.0
        },
        {
            "name": "Ackley",
            "func": ackley,
            "bounds": (-32.768, 32.768),
            "optimal": 0.0
        },
        {
            "name": "Rosenbrock",
            "func": rosenbrock,
            "bounds": (-5, 10),
            "optimal": 0.0
        },
        {
            "name": "Schwefel",
            "func": schwefel,
            "bounds": (-500, 500),
            "optimal": 0.0
        }
    ]
    
    # Run tests
    print(f"\nTest Configuration:")
    print(f"  Dimension: {dim}")
    print(f"  Population Size: {pop_size} (10√óD)")
    print(f"  Max Iterations: {max_iter}")
    print(f"  Mutation Factor (F): {F}")
    print(f"  Crossover Rate (CR): {CR}")
    print(f"  Strategy: DE/rand/1/bin")
    print(f"\n  NOTE: Using STANDARD parameters (F=0.5, CR=0.9)")
    print(f"        to demonstrate classical DE behavior and limitations.")
    print()
    
    results_summary = []
    
    for benchmark in benchmarks_to_test:
        print("\n" + "=" * 70)
        print(f"Testing: {benchmark['name']} Function")
        print("=" * 70)
        print(f"Search Space: {benchmark['bounds']}")
        print(f"Global Optimum: {benchmark['optimal']}")
        print("-" * 70)
        
        # Run DE
        best_solution, best_fitness, convergence = de(
            objective_func=benchmark['func'],
            dim=dim,
            bounds=benchmark['bounds'],
            pop_size=pop_size,
            max_iter=max_iter,
            F=F,
            CR=CR
        )
        
        print("-" * 70)
        print(f"\nüìä RESULTS for {benchmark['name']}:")
        print(f"  Best Fitness Found: {best_fitness:.10e}")
        print(f"  Global Optimum:     {benchmark['optimal']:.10e}")
        print(f"  Error from Optimum: {abs(best_fitness - benchmark['optimal']):.10e}")
        print(f"  Best Solution (first 5 dims): {best_solution[:5]}")
        
        # Convergence analysis
        initial_fitness = convergence[0]
        final_fitness = convergence[-1]
        
        print(f"\nüìà CONVERGENCE ANALYSIS:")
        print(f"  Initial Best:  {initial_fitness:.6e}")
        print(f"  Final Best:    {final_fitness:.6e}")
        
        if final_fitness != 0:
            improvement = initial_fitness / final_fitness
            print(f"  Improvement:   {improvement:.2e}x better")
        else:
            print(f"  Improvement:   Optimal solution found!")
        
        # Check convergence quality
        error = abs(best_fitness - benchmark['optimal'])
        if error < 1e-6:
            quality = "EXCELLENT ‚úÖ"
        elif error < 1e-3:
            quality = "GOOD ‚úì"
        elif error < 1e-1:
            quality = "ACCEPTABLE ~"
        else:
            quality = "POOR ‚úó"
        
        print(f"  Quality:       {quality}")
        
        results_summary.append({
            'function': benchmark['name'],
            'fitness': best_fitness,
            'error': error,
            'quality': quality
        })
    
    # Summary table
    print("\n" + "=" * 70)
    print(" " * 25 + "SUMMARY TABLE")
    print("=" * 70)
    print(f"{'Function':<15} {'Best Fitness':>15} {'Error':>15} {'Quality':>15}")
    print("-" * 70)
    
    for result in results_summary:
        print(f"{result['function']:<15} {result['fitness']:>15.6e} "
              f"{result['error']:>15.6e} {result['quality']:>15}")
    
    print("=" * 70)
    
    # Analysis note
    print("\nüìù PERFORMANCE ANALYSIS:")
    print("-" * 70)
    print("Classical DE (F=0.5, CR=0.9) Performance:")
    print("  ‚úÖ GOOD on unimodal:     Sphere, Ackley, Rosenbrock")
    print("  ‚ö†Ô∏è  POOR on multimodal:   Rastrigin, Schwefel")
    print("\nThis demonstrates the need for:")
    print("  ‚Ä¢ Parameter tuning for different problem types")
    print("  ‚Ä¢ Modern adaptive algorithms")
    print("  ‚Ä¢ Advanced optimization techniques")
    print("-" * 70)
    
    print("\nüí° NOTE FOR PAPER:")
    print("-" * 70)
    print("These results with STANDARD parameters demonstrate that")
    print("classical algorithms have limitations on complex multimodal")
    print("problems, motivating the development of modern metaheuristics.")
    print("-" * 70)
    
    print("\n‚úÖ DE Algorithm testing complete!")
    print("=" * 70)
